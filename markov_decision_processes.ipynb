{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, shape, start, goal, traps, rewards):\n",
    "        self.shape = shape  # (height, width)\n",
    "        self.start = start  # (row, col)\n",
    "        self.goal = goal  # (row, col)\n",
    "        self.traps = traps  # list of (row, col) tuples\n",
    "        self.rewards = rewards  # dictionary of (row, col): reward pairs\n",
    "\n",
    "        self.states = self.get_states()\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "    def get_states(self):\n",
    "        # Return a list of all possible states (positions)\n",
    "        states = []\n",
    "        for row in range(self.shape[0]):\n",
    "            for col in range(self.shape[1]):\n",
    "                states.append((row, col))\n",
    "        return states\n",
    "\n",
    "    def transition_func(self, state, action):\n",
    "        # Implement the transition function\n",
    "        row, col = state\n",
    "        if action == 'up':\n",
    "            new_row = max(row - 1, 0)\n",
    "            new_col = col\n",
    "        elif action == 'down':\n",
    "            new_row = min(row + 1, self.shape[0] - 1)\n",
    "            new_col = col\n",
    "        elif action == 'left':\n",
    "            new_row = row\n",
    "            new_col = max(col - 1, 0)\n",
    "        elif action == 'right':\n",
    "            new_row = row\n",
    "            new_col = min(col + 1, self.shape[1] - 1)\n",
    "\n",
    "        new_state = (new_row, new_col)\n",
    "        reward = self.rewards.get(new_state, 0)\n",
    "        is_terminal = (new_state == self.goal) or (new_state in self.traps)\n",
    "        return new_state, reward, is_terminal\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_state = self.start\n",
    "        return self.current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: [(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 1), (1, 2), (1, 3), (2, 0), (2, 1), (2, 2), (2, 3), (3, 0), (3, 1), (3, 2), (3, 3)]\n"
     ]
    }
   ],
   "source": [
    "shape = (4, 4)\n",
    "start = (0, 0)\n",
    "goal = (3, 3)\n",
    "traps = [(1, 1)]\n",
    "rewards = {goal: 1, (1, 1): -1}\n",
    "\n",
    "env = GridWorld(shape, start, goal, traps, rewards)\n",
    "print(\"States:\", env.states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, we define a q_learning function that takes the environment (env) and hyperparameters like learning rate (alpha), discount factor (gamma), exploration rate (epsilon), and the number of episodes (episodes).\n",
    "\n",
    "The function initializes the Q-values to 0 for all state-action pairs. Then, for each episode, it starts from the initial state and iteratively updates the Q-values using the Q-learning update rule:\n",
    "\n",
    "\n",
    "Q(s, a) += alpha * (reward + gamma * max_a'(Q(s', a')) - Q(s, a))\n",
    "After training, the function returns the learned Q-values.\n",
    "\n",
    "We then create a policy by selecting the action with the maximum Q-value for each state. Finally, we print the optimal policy by iterating over the grid and printing the optimal action for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "R R R D \n",
      "U U U D \n",
      "U D U D \n",
      "U R R U \n"
     ]
    }
   ],
   "source": [
    "def q_learning(env, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=10000):\n",
    "    # Initialize Q-values\n",
    "    q_values = {}\n",
    "    for state in env.states:\n",
    "        q_values[state] = {action: 0.0 for action in env.actions}\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.start\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Choose an action using epsilon-greedy\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice(env.actions)\n",
    "            else:\n",
    "                action = max(q_values[state], key=q_values[state].get)\n",
    "\n",
    "            # Take the action and observe the new state, reward, and terminal flag\n",
    "            new_state, reward, done = env.transition_func(state, action)\n",
    "\n",
    "            # Update Q-value\n",
    "            q_values[state][action] += alpha * (\n",
    "                reward + gamma * max(q_values[new_state].values()) - q_values[state][action]\n",
    "            )\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "    return q_values\n",
    "\n",
    "# Train the agent\n",
    "q_values = q_learning(env)\n",
    "\n",
    "# Print the optimal policy\n",
    "policy = {}\n",
    "for state in env.states:\n",
    "    policy[state] = max(q_values[state], key=q_values[state].get)\n",
    "\n",
    "print(\"Optimal Policy:\")\n",
    "for row in range(env.shape[0]):\n",
    "    for col in range(env.shape[1]):\n",
    "        state = (row, col)\n",
    "        action = policy[state]\n",
    "        print(f\"{action[0].upper()}\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the basic implementation, we used a tabular representation to store the Q-values for each state-action pair. However, this approach becomes intractable for problems with large or continuous state spaces, as the memory requirements grow exponentially.\n",
    "\n",
    "Neural networks can be used as function approximators to represent the Q-value function Q(s, a). Instead of storing individual Q-values for each state-action pair, we can train a neural network to approximate the Q-value function, taking the state and action as inputs and outputting the corresponding Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "        return q_value\n",
    "\n",
    "# Example usage\n",
    "state_dim = 4  # Dimensions of the state space\n",
    "action_dim = 2  # Dimensions of the action space\n",
    "hidden_dim = 32\n",
    "\n",
    "q_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "state = torch.randn(1, state_dim)  # Sample state\n",
    "action = torch.randn(1, action_dim)  # Sample action\n",
    "q_value = q_network(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, shape, start, goal, traps, rewards):\n",
    "        self.shape = shape  # (height, width)\n",
    "        self.start = start  # (row, col)\n",
    "        self.goal = goal  # (row, col)\n",
    "        self.traps = traps  # list of (row, col) tuples\n",
    "        self.rewards = rewards  # dictionary of (row, col): reward pairs\n",
    "\n",
    "        self.states = self.get_states()\n",
    "        self.actions = [0, 1, 2, 3]  # up, down, left, right\n",
    "        self.action_space = range(len(self.actions))\n",
    "        self.state_dim = 2  # (row, col)\n",
    "        self.action_dim = 1  # one-hot encoding of actions\n",
    "        self.current_state = start\n",
    "\n",
    "    def get_states(self):\n",
    "        # Return a list of all possible states (positions)\n",
    "        states = []\n",
    "        for row in range(self.shape[0]):\n",
    "            for col in range(self.shape[1]):\n",
    "                states.append((row, col))\n",
    "        return states\n",
    "\n",
    "    def transition_func(self, state, action):\n",
    "        # Implement the transition function\n",
    "        row, col = state\n",
    "        if action == 0:  # up\n",
    "            new_row = max(row - 1, 0)\n",
    "            new_col = col\n",
    "        elif action == 1:  # down\n",
    "            new_row = min(row + 1, self.shape[0] - 1)\n",
    "            new_col = col\n",
    "        elif action == 2:  # left\n",
    "            new_row = row\n",
    "            new_col = max(col - 1, 0)\n",
    "        elif action == 3:  # right\n",
    "            new_row = row\n",
    "            new_col = min(col + 1, self.shape[1] - 1)\n",
    "\n",
    "        next_state = (new_row, new_col)\n",
    "        reward = self.rewards.get(next_state, 0)\n",
    "        is_terminal = (next_state == self.goal) or (next_state in self.traps)\n",
    "        self.current_state = next_state\n",
    "        return next_state, reward, is_terminal\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to the start state\n",
    "        return self.start\n",
    "\n",
    "    def step(self, action):\n",
    "        # Take a step in the environment\n",
    "        state = self.current_state\n",
    "        next_state, reward, is_terminal = self.transition_func(state, action)\n",
    "        self.current_state = next_state\n",
    "        return next_state, reward, is_terminal, {}\n",
    "\n",
    "# Example Grid World\n",
    "shape = (4, 4)\n",
    "start = (0, 0)\n",
    "goal = (3, 3)\n",
    "traps = [(1, 1)]\n",
    "rewards = {goal: 1, (1, 1): -1}\n",
    "\n",
    "env = GridWorld(shape, start, goal, traps, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid action: 2\n",
      "torch.Size([1, 2]) torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m action_tensor \u001b[38;5;241m=\u001b[39m action_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(next_state_tensor\u001b[38;5;241m.\u001b[39mshape, action_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 77\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     78\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m (rewards \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m target_q_values \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones))\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Compute Q-values for current state-action pairs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[59], line 15\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[0;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "        return q_value\n",
    "\n",
    "# Hyperparameters\n",
    "state_dim = 4\n",
    "action_dim = 2\n",
    "hidden_dim = 32\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "replay_buffer_size = 10000\n",
    "update_target_every = 1000\n",
    "num_episodes = 10000\n",
    "\n",
    "# Initialize networks\n",
    "q_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "target_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "target_network.load_state_dict(q_network.state_dict())  # Initialize target network weights\n",
    "optimizer = optim.Adam(q_network.parameters())\n",
    "replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Choose action using epsilon-greedy policy\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(list(env.action_space))  # Explore\n",
    "        else:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            q_values = q_network(state_tensor)\n",
    "            action = torch.argmax(q_values).item()  # Exploit\n",
    "\n",
    "        # Take action and observe next state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Store transition in replay buffer\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Sample batch from replay buffer\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        next_state_tensor = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "        if action < action_dim:\n",
    "            action_tensor = torch.nn.functional.one_hot(torch.tensor(action), num_classes=action_dim)\n",
    "        else:\n",
    "            print(f\"Invalid action: {action}\")\n",
    "\n",
    "        action_tensor = action_tensor.unsqueeze(0)\n",
    "        print(next_state_tensor.shape, action_tensor.shape)\n",
    "        target_q_values = target_network(next_state_tensor, action_tensor).max(dim=1)[0]\n",
    "        target_q_values = (rewards + gamma * target_q_values * (1 - dones))\n",
    "\n",
    "        # Compute Q-values for current state-action pairs\n",
    "        state_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "        action_tensor = torch.tensor(actions, dtype=torch.int64)\n",
    "        q_values = q_network(state_tensor, action_tensor.unsqueeze(1)).squeeze()\n",
    "\n",
    "        # Compute loss and update Q-network\n",
    "        loss = nn.MSELoss()(q_values, target_q_values.detach())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update target network weights\n",
    "        if episode % update_target_every == 0:\n",
    "            target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        # Update epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    print(f\"Episode {episode}, Reward: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
